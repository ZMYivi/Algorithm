# 深度学习-吴恩达
## 神经网络与深度学习

## 1. 深度学习概论

### 1.2 什么是神经网络？

1. “深度学习”指的是训练神经网络
2. 修正线性单元（ReLU）
3. 监督学习

### 1.3 用神经网络进行监督学习

1. 神经网络的使用：
   1. 房地产、广告：标准的NN
   2. 图像处理：CNN
   3. 序列数据（音频）：RNN（循环神经网络）
   4. 语言（英语、汉语，也可理解为一个序列数据）：RNNs
   5. 更复杂的应用（无人驾驶）：CNN“卷积神经网络结构”架构、混合神经网络

2. 结构化数据：数据库中的数据，有着固定的格式

3. 非结构化数据：音频、图形等等（人类擅长理解非结构化的数据，细品）

### 1.4 为什么深度学习会兴起？

1. 在传统的算法中，例如：向量机、逻辑回归等等，算法的性能会随着数据的提高而提高，但是随着数据量持续变大，性能并不会继续的提高了，简单的神经网络、中等的神经网络、大型的神经网络，随着大小的变化，算法的性能也会持续的变大，且远高于传统的算法
2. 规模一直在推动深度学习的进步
3. 让算法变得更快，迭代的速度更快

## 2. 神经网络基础

### 2.1 二分分类

1. n表示特征向量的维度
2. 对于一个图片，在计算机中存储的方式是一个三个矩阵，分别是红黄蓝，如果要把一个图片转换为特征向量，那么就需要把每一个颜色的矩阵转化为一个一维的向量，然后把三个颜色的向量放在一起，就是一个特征向量
3. 在表示训练的集的时候，输入数据通常表示为一个矩阵，其中每一个输入数据通过一个列向量来组合成一个矩阵，有时候也用行向量来表示，但是通常是列向量的组合方式
4. 输出数据是一个行向量

### 2.2 logistics回归

1. 激活函数：
   $$
   sigmoid(z) = \frac{1}{1+e^{-z}}
   $$

2. 给定输入的数据x，和参数w，已经常量b，其中x和w都是维度相同的向量，那么可以得到算式为：
   $$
   y=w^T*x+b
   $$
   但是作为一个二分回归，其结果需要是1或者是0，但是如果根据上述的公式计算的话，得到的值并不会刚好等于0或者1，而且一些很奇怪的数字，在这里我们规定输出的值是一个概率值：
   $$
   \hat{y}=P(y=1|x)
   $$
   即当x满足某种条件的时候使得y的值等于1，如果得到的是别的数字是没有用的，因此这里就引入了激活函数使得结果可以被映射到0到1中，这个函数的定义域和值域分别为负无穷到正无穷和0到1，其中当x为0的时候，y的值为0.5

### 2.3 logistic回归损失函数

1. Loss function，定位为y-hat和y的差的平方或者他们差的平方的1/2，来衡量你的预测输出值y-hat和y的实际值有多么接近，在逻辑回归中用到的损失函数为：
   $$
   \delta(y,\hat{y})=-(y*log(\hat{y})+(1-y)*log(1-\hat{y}))
   $$
   这样写函数的原因是，因为是一个二分类，所以正确的值只有0和1，也就是说y只取值0和1，分别代入上面的式子中，可以得到，当y为1时，y-hat的值需要尽可能的大，因为激活函数，所以y-hat最大也就只能无限接近于1，同理当y为0时，y-hat无限小，无限接近于0

2. Cost function，成本函数就是损失函数的平均值

### 2.4 梯度下降法

1. 凸函数

2. 梯度下降就是不停的迭代，重复：
   $$
   w:=w-\alpha{\frac {dJ(w)}{dw}}
   $$
   其中埃尔法是学习率，学习率可以控制每次梯度下降法中的步长

3. 注意：导数符号和偏导数符号一个是小写的d，另一个是花体的d

### 2.5 导数

### 2.6 更多导数的例子    

### 2.7 计算图

### 2.8 计算图的导数计算

